<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An R Platform for Social Scientists</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="R book for social scientists">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="An R Platform for Social Scientists" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/coverpicture.png" />
  <meta property="og:description" content="R book for social scientists" />
  <meta name="github-repo" content="burakaydin/SARP-EN" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An R Platform for Social Scientists" />
  
  <meta name="twitter:description" content="R book for social scientists" />
  <meta name="twitter:image" content="images/coverpicture.png" />

<meta name="author" content="Burak AYDIN, James ALGINA, Walter LEITE">


<meta name="date" content="2017-01-01">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="correlation.html">
<link rel="next" href="useful-r-codes.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<link href="libs/rglwidgetClass-2/rgl.css" rel="stylesheet" />
<script src="libs/rglwidgetClass-2/rglClass.src.js"></script>
<script src="libs/CanvasMatrix4-2016/CanvasMatrix.src.js"></script>
<script src="libs/rglWebGL-binding-0.98.1/rglWebGL.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">SARP-EN</a></li>
<li><a href="https://bookdown.org/burak2358/SARP-TR/" target="blank">SARP-TR</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Cover</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#why-bookdown"><i class="fa fa-check"></i><b>1.1.1</b> Why Bookdown?</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#content"><i class="fa fa-check"></i><b>1.1.2</b> Content</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i><b>2</b> Preface</a><ul>
<li class="chapter" data-level="2.1" data-path="preface.html"><a href="preface.html#authors"><i class="fa fa-check"></i><b>2.1</b> Authors</a><ul>
<li class="chapter" data-level="2.1.1" data-path="preface.html"><a href="preface.html#burak-aydn-ph.d."><i class="fa fa-check"></i><b>2.1.1</b> Burak Aydın, Ph.D.</a></li>
<li class="chapter" data-level="2.1.2" data-path="preface.html"><a href="preface.html#james-algina-ph.d."><i class="fa fa-check"></i><b>2.1.2</b> James Algina, Ph.D.</a></li>
<li class="chapter" data-level="2.1.3" data-path="preface.html"><a href="preface.html#walter-l.-leite-ph.d."><i class="fa fa-check"></i><b>2.1.3</b> Walter L. Leite, Ph.D.</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="preface.html"><a href="preface.html#acknowledgement"><i class="fa fa-check"></i><b>2.2</b> Acknowledgement</a></li>
<li class="chapter" data-level="2.3" data-path="preface.html"><a href="preface.html#dataWBT"><i class="fa fa-check"></i><b>2.3</b> Data</a></li>
<li class="chapter" data-level="2.4" data-path="preface.html"><a href="preface.html#fund"><i class="fa fa-check"></i><b>2.4</b> Fund</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="rs-popularity.html"><a href="rs-popularity.html"><i class="fa fa-check"></i><b>3</b> R’s Popularity</a></li>
<li class="chapter" data-level="4" data-path="setting-up-r-for-windows.html"><a href="setting-up-r-for-windows.html"><i class="fa fa-check"></i><b>4</b> Setting up R for Windows</a></li>
<li class="chapter" data-level="5" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>5</b> Basics</a><ul>
<li class="chapter" data-level="5.1" data-path="basics.html"><a href="basics.html#functions"><i class="fa fa-check"></i><b>5.1</b> Functions</a><ul>
<li class="chapter" data-level="5.1.1" data-path="basics.html"><a href="basics.html#r-as-a-basic-calculator."><i class="fa fa-check"></i><b>5.1.1</b> R as a Basic Calculator.</a></li>
<li class="chapter" data-level="5.1.2" data-path="basics.html"><a href="basics.html#r-as-a-programmable-calculator"><i class="fa fa-check"></i><b>5.1.2</b> R as a Programmable Calculator</a></li>
<li class="chapter" data-level="5.1.3" data-path="basics.html"><a href="basics.html#help"><i class="fa fa-check"></i><b>5.1.3</b> Help!</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="basics.html"><a href="basics.html#r-data-types"><i class="fa fa-check"></i><b>5.2</b> R Data Types</a><ul>
<li class="chapter" data-level="5.2.1" data-path="basics.html"><a href="basics.html#vectors"><i class="fa fa-check"></i><b>5.2.1</b> Vectors</a></li>
<li class="chapter" data-level="5.2.2" data-path="basics.html"><a href="basics.html#matricies"><i class="fa fa-check"></i><b>5.2.2</b> Matricies</a></li>
<li class="chapter" data-level="5.2.3" data-path="basics.html"><a href="basics.html#variables"><i class="fa fa-check"></i><b>5.2.3</b> Variables</a></li>
<li class="chapter" data-level="5.2.4" data-path="basics.html"><a href="basics.html#factors"><i class="fa fa-check"></i><b>5.2.4</b> Factors</a></li>
<li class="chapter" data-level="5.2.5" data-path="basics.html"><a href="basics.html#missing-values"><i class="fa fa-check"></i><b>5.2.5</b> Missing Values</a></li>
<li class="chapter" data-level="5.2.6" data-path="basics.html"><a href="basics.html#dataframes"><i class="fa fa-check"></i><b>5.2.6</b> Data Frames</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="basics.html"><a href="basics.html#r-packages"><i class="fa fa-check"></i><b>5.3</b> R Packages</a></li>
<li class="chapter" data-level="5.4" data-path="basics.html"><a href="basics.html#theworkspace"><i class="fa fa-check"></i><b>5.4</b> The Workspace</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-sets.html"><a href="data-sets.html"><i class="fa fa-check"></i><b>6</b> Data Sets</a><ul>
<li class="chapter" data-level="6.1" data-path="data-sets.html"><a href="data-sets.html#import-data"><i class="fa fa-check"></i><b>6.1</b> Import Data</a><ul>
<li class="chapter" data-level="6.1.1" data-path="data-sets.html"><a href="data-sets.html#csv"><i class="fa fa-check"></i><b>6.1.1</b> CSV</a></li>
<li class="chapter" data-level="6.1.2" data-path="data-sets.html"><a href="data-sets.html#spss"><i class="fa fa-check"></i><b>6.1.2</b> SPSS</a></li>
<li class="chapter" data-level="6.1.3" data-path="data-sets.html"><a href="data-sets.html#rdata"><i class="fa fa-check"></i><b>6.1.3</b> Rdata</a></li>
<li class="chapter" data-level="6.1.4" data-path="data-sets.html"><a href="data-sets.html#pullonline"><i class="fa fa-check"></i><b>6.1.4</b> Pull online</a></li>
<li class="chapter" data-level="6.1.5" data-path="data-sets.html"><a href="data-sets.html#read-data-through-r-studio"><i class="fa fa-check"></i><b>6.1.5</b> Read data through R studio</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="data-sets.html"><a href="data-sets.html#basic-data-manipulation"><i class="fa fa-check"></i><b>6.2</b> Basic Data Manipulation</a><ul>
<li class="chapter" data-level="6.2.1" data-path="data-sets.html"><a href="data-sets.html#replacing-values"><i class="fa fa-check"></i><b>6.2.1</b> Replacing values</a></li>
<li class="chapter" data-level="6.2.2" data-path="data-sets.html"><a href="data-sets.html#subsetting"><i class="fa fa-check"></i><b>6.2.2</b> Subsetting</a></li>
<li class="chapter" data-level="6.2.3" data-path="data-sets.html"><a href="data-sets.html#creating-new-variables"><i class="fa fa-check"></i><b>6.2.3</b> Creating new variables</a></li>
<li class="chapter" data-level="6.2.4" data-path="data-sets.html"><a href="data-sets.html#reshaping-data"><i class="fa fa-check"></i><b>6.2.4</b> Reshaping data</a></li>
<li class="chapter" data-level="6.2.5" data-path="data-sets.html"><a href="data-sets.html#converting-between-variable-types"><i class="fa fa-check"></i><b>6.2.5</b> Converting between variable types</a></li>
<li class="chapter" data-level="6.2.6" data-path="data-sets.html"><a href="data-sets.html#delete-cases"><i class="fa fa-check"></i><b>6.2.6</b> Delete cases</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="data-sets.html"><a href="data-sets.html#export-data"><i class="fa fa-check"></i><b>6.3</b> Export Data</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html"><i class="fa fa-check"></i><b>7</b> Descriptive Statistics and Hypthoses Testing</a><ul>
<li class="chapter" data-level="7.1" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#descstats"><i class="fa fa-check"></i><b>7.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="7.1.1" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#mean"><i class="fa fa-check"></i><b>7.1.1</b> Mean</a></li>
<li class="chapter" data-level="7.1.2" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#median"><i class="fa fa-check"></i><b>7.1.2</b> Median</a></li>
<li class="chapter" data-level="7.1.3" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#variance"><i class="fa fa-check"></i><b>7.1.3</b> Variance</a></li>
<li class="chapter" data-level="7.1.4" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#standard-deviation"><i class="fa fa-check"></i><b>7.1.4</b> Standard deviation</a></li>
<li class="chapter" data-level="7.1.5" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#skewness"><i class="fa fa-check"></i><b>7.1.5</b> Skewness</a></li>
<li class="chapter" data-level="7.1.6" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#kurtosis"><i class="fa fa-check"></i><b>7.1.6</b> Kurtosis</a></li>
<li class="chapter" data-level="7.1.7" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#reporting-descriptives"><i class="fa fa-check"></i><b>7.1.7</b> Reporting descriptives</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#basic-graphics"><i class="fa fa-check"></i><b>7.2</b> Basic graphics</a><ul>
<li class="chapter" data-level="7.2.1" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#histogram"><i class="fa fa-check"></i><b>7.2.1</b> Histogram</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#hypothesis-testing-introduction"><i class="fa fa-check"></i><b>7.3</b> Hypothesis testing introduction</a><ul>
<li class="chapter" data-level="7.3.1" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#samplingdist"><i class="fa fa-check"></i><b>7.3.1</b> The Sampling distribution</a></li>
<li class="chapter" data-level="7.3.2" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#the-confidence-intervals-ci"><i class="fa fa-check"></i><b>7.3.2</b> The Confidence Intervals (CI)</a></li>
<li class="chapter" data-level="7.3.3" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#the-null-hypothesis"><i class="fa fa-check"></i><b>7.3.3</b> The null hypothesis</a></li>
<li class="chapter" data-level="7.3.4" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#the-z-score-and-the-z-test"><i class="fa fa-check"></i><b>7.3.4</b> The z score and the z test</a></li>
<li class="chapter" data-level="7.3.5" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#the-one-sample-t-test"><i class="fa fa-check"></i><b>7.3.5</b> The one-sample t test</a></li>
<li class="chapter" data-level="7.3.6" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#the-p-value"><i class="fa fa-check"></i><b>7.3.6</b> The p value</a></li>
<li class="chapter" data-level="7.3.7" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#the-p-value-illustration"><i class="fa fa-check"></i><b>7.3.7</b> The p value illustration</a></li>
<li class="chapter" data-level="7.3.8" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#statisticalpower"><i class="fa fa-check"></i><b>7.3.8</b> Statistical power</a></li>
<li class="chapter" data-level="7.3.9" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#in-case-the-z-and-the-t-distribution-is-not-valid"><i class="fa fa-check"></i><b>7.3.9</b> In case the z and the t distribution is not valid</a></li>
<li class="chapter" data-level="7.3.10" data-path="descriptive-statistics-and-hypthoses-testing.html"><a href="descriptive-statistics-and-hypthoses-testing.html#shiny-application-to-visualize-sampling-distribution"><i class="fa fa-check"></i><b>7.3.10</b> Shiny application to visualize sampling distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html"><i class="fa fa-check"></i><b>8</b> Comparing Two Means, the t-test</a><ul>
<li class="chapter" data-level="8.1" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#between-subjects-t-test-the-independent-groups-t-test"><i class="fa fa-check"></i><b>8.1</b> Between-Subjects t-test (The Independent Groups t-test)</a><ul>
<li class="chapter" data-level="8.1.1" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#r-codes-for-the-independent-groups-t-test"><i class="fa fa-check"></i><b>8.1.1</b> R codes for the independent groups t-test</a></li>
<li class="chapter" data-level="8.1.2" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#assumptions-of-the-independent-groups-t-test"><i class="fa fa-check"></i><b>8.1.2</b> Assumptions of the independent groups t-test</a></li>
<li class="chapter" data-level="8.1.3" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#using-welchs-t-test"><i class="fa fa-check"></i><b>8.1.3</b> Using Welch’s t test</a></li>
<li class="chapter" data-level="8.1.4" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#indepteff"><i class="fa fa-check"></i><b>8.1.4</b> Effect size for the independent groups t-test</a></li>
<li class="chapter" data-level="8.1.5" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#extra-practical-significance-vs-statistical-significance"><i class="fa fa-check"></i><b>8.1.5</b> Extra: Practical significance vs statistical significance</a></li>
<li class="chapter" data-level="8.1.6" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#missing-data-techniques-for-the-independent-groups-t-test"><i class="fa fa-check"></i><b>8.1.6</b> Missing data techniques for the independent groups t-test</a></li>
<li class="chapter" data-level="8.1.7" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#supportive-graphs-for-the-independent-groups-t-test"><i class="fa fa-check"></i><b>8.1.7</b> Supportive graphs for the independent groups t-test</a></li>
<li class="chapter" data-level="8.1.8" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#power-calculations-for-the-independent-groups-t-test"><i class="fa fa-check"></i><b>8.1.8</b> Power calculations for the independent groups t-test</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#the-dependent-groups-t-test-within-subjects-t-test"><i class="fa fa-check"></i><b>8.2</b> The dependent groups t-test (Within-subjects t-test)</a><ul>
<li class="chapter" data-level="8.2.1" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#r-codes-for-the-dependent-groups-t-test"><i class="fa fa-check"></i><b>8.2.1</b> R codes for the dependent groups t-test</a></li>
<li class="chapter" data-level="8.2.2" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#assumption-for-the-dependent-groups-t-test"><i class="fa fa-check"></i><b>8.2.2</b> Assumption for the dependent groups t-test</a></li>
<li class="chapter" data-level="8.2.3" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#robust-estimation-for-the-dependent-groups-t-test"><i class="fa fa-check"></i><b>8.2.3</b> Robust estimation for the dependent groups t-test</a></li>
<li class="chapter" data-level="8.2.4" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#effect-size-for-the-dependent-groups-t-test"><i class="fa fa-check"></i><b>8.2.4</b> Effect size for the dependent groups t-test</a></li>
<li class="chapter" data-level="8.2.5" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#missing-data-techniques-for-the-dependent-groups-t-test"><i class="fa fa-check"></i><b>8.2.5</b> Missing data techniques for the dependent groups t-test</a></li>
<li class="chapter" data-level="8.2.6" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#supportive-graphs-for-the-dependent-groups-t-test"><i class="fa fa-check"></i><b>8.2.6</b> Supportive graphs for the dependent groups t-test</a></li>
<li class="chapter" data-level="8.2.7" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#power-calculations-for-the-dependent-groups-t-test"><i class="fa fa-check"></i><b>8.2.7</b> Power calculations for the dependent groups t-test</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#common-designs"><i class="fa fa-check"></i><b>8.3</b> Common Designs</a><ul>
<li class="chapter" data-level="8.3.1" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#designs-in-which-scores-in-the-two-treatments-are-correlated"><i class="fa fa-check"></i><b>8.3.1</b> Designs in which Scores in the Two Treatments are Correlated</a></li>
<li class="chapter" data-level="8.3.2" data-path="comparing-two-means-the-t-test.html"><a href="comparing-two-means-the-t-test.html#designbetween"><i class="fa fa-check"></i><b>8.3.2</b> Designs in which Scores in the Two Treatments are Independent</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>9</b> Analysis of Variance (ANOVA)</a><ul>
<li class="chapter" data-level="9.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#terminology"><i class="fa fa-check"></i><b>9.1</b> Terminology</a></li>
<li class="chapter" data-level="9.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#between-subjects-anova"><i class="fa fa-check"></i><b>9.2</b> Between Subjects ANOVA</a><ul>
<li class="chapter" data-level="9.2.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#one-way-between-subjects-anova"><i class="fa fa-check"></i><b>9.2.1</b> One-way Between Subjects ANOVA</a></li>
<li class="chapter" data-level="9.2.2" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#two-factor-between-subjects-anova"><i class="fa fa-check"></i><b>9.2.2</b> Two-Factor Between Subjects ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#within-subjects-anova"><i class="fa fa-check"></i><b>9.3</b> Within Subjects ANOVA</a><ul>
<li class="chapter" data-level="9.3.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#one-way-within-subjects-anova"><i class="fa fa-check"></i><b>9.3.1</b> One-way Within-Subjects ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#mixed-design"><i class="fa fa-check"></i><b>9.4</b> Mixed Design</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="correlation.html"><a href="correlation.html"><i class="fa fa-check"></i><b>10</b> Correlation</a><ul>
<li class="chapter" data-level="10.1" data-path="correlation.html"><a href="correlation.html#pearson-correlation-coefficient"><i class="fa fa-check"></i><b>10.1</b> Pearson correlation coefficient</a><ul>
<li class="chapter" data-level="10.1.1" data-path="correlation.html"><a href="correlation.html#inference-on-a-pearson-correlation-coefficient"><i class="fa fa-check"></i><b>10.1.1</b> Inference on a Pearson correlation coefficient</a></li>
<li class="chapter" data-level="10.1.2" data-path="correlation.html"><a href="correlation.html#r-codes-for-pearson-correlation-coefficent"><i class="fa fa-check"></i><b>10.1.2</b> R codes for Pearson Correlation coefficent</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="correlation.html"><a href="correlation.html#spearmans-rho-and-kendalls-tau"><i class="fa fa-check"></i><b>10.2</b> Spearman’s rho and Kendall’s tau</a><ul>
<li class="chapter" data-level="10.2.1" data-path="correlation.html"><a href="correlation.html#the-r-code-for-spearmans-rho-and-kendalls-tau"><i class="fa fa-check"></i><b>10.2.1</b> The R code for Spearman’s rho and Kendall’s tau</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="correlation.html"><a href="correlation.html#biserial-and-point-biserial-correlation-coefficients-with-r"><i class="fa fa-check"></i><b>10.3</b> Biserial and Point-Biserial Correlation Coefficients with R</a></li>
<li class="chapter" data-level="10.4" data-path="correlation.html"><a href="correlation.html#phi-correlation-coefficient-with-r"><i class="fa fa-check"></i><b>10.4</b> Phi Correlation Coefficient with R</a></li>
<li class="chapter" data-level="10.5" data-path="correlation.html"><a href="correlation.html#issues-in-interpreting-correlation-coefficients"><i class="fa fa-check"></i><b>10.5</b> Issues in Interpreting Correlation Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html"><i class="fa fa-check"></i><b>11</b> Multiple Linear Regression, A Short Introduction</a><ul>
<li class="chapter" data-level="11.1" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#matricies-and-least-square-estimation"><i class="fa fa-check"></i><b>11.1</b> Matricies and Least Square Estimation</a><ul>
<li class="chapter" data-level="11.1.1" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#a-essentially-all-models-are-wrong-but-some-are-useful."><i class="fa fa-check"></i><b>11.1.1</b> a) “Essentially, all models are wrong, but some are useful.”</a></li>
<li class="chapter" data-level="11.1.2" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#b-strength-of-relationship-between-the-dependent-and-independent-variables"><i class="fa fa-check"></i><b>11.1.2</b> b) Strength of relationship between the dependent and independent variables</a></li>
<li class="chapter" data-level="11.1.3" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#c-residuals-and-influential-data-points"><i class="fa fa-check"></i><b>11.1.3</b> c) Residuals and influential data points</a></li>
<li class="chapter" data-level="11.1.4" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#d-equal-variance-assumption"><i class="fa fa-check"></i><b>11.1.4</b> d) <em>Equal variance assumption</em></a></li>
<li class="chapter" data-level="11.1.5" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#e-hypothesis-testing"><i class="fa fa-check"></i><b>11.1.5</b> e) Hypothesis testing</a></li>
<li class="chapter" data-level="11.1.6" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#f-variable-selection"><i class="fa fa-check"></i><b>11.1.6</b> f) Variable Selection</a></li>
<li class="chapter" data-level="11.1.7" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#g-colliniearity"><i class="fa fa-check"></i><b>11.1.7</b> g) Colliniearity</a></li>
<li class="chapter" data-level="11.1.8" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#h-non-linearity"><i class="fa fa-check"></i><b>11.1.8</b> h) Non-linearity</a></li>
<li class="chapter" data-level="11.1.9" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#i-correlated-errors-and-nonindependent-errors"><i class="fa fa-check"></i><b>11.1.9</b> i) Correlated errors and nonindependent errors</a></li>
<li class="chapter" data-level="11.1.10" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#j-centering-and-scaling"><i class="fa fa-check"></i><b>11.1.10</b> j) Centering and Scaling</a></li>
<li class="chapter" data-level="11.1.11" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#k-standardized-coefficients"><i class="fa fa-check"></i><b>11.1.11</b> k) Standardized coefficients</a></li>
<li class="chapter" data-level="11.1.12" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#l-interactions"><i class="fa fa-check"></i><b>11.1.12</b> l) Interactions</a></li>
<li class="chapter" data-level="11.1.13" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#m-estimators"><i class="fa fa-check"></i><b>11.1.13</b> m) Estimators</a></li>
<li class="chapter" data-level="11.1.14" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#n-robust-regression"><i class="fa fa-check"></i><b>11.1.14</b> n) Robust Regression</a></li>
<li class="chapter" data-level="11.1.15" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#o-sample-size-and-statistical-power"><i class="fa fa-check"></i><b>11.1.15</b> o) Sample size and statistical power</a></li>
<li class="chapter" data-level="11.1.16" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#p-reliability-of-variables"><i class="fa fa-check"></i><b>11.1.16</b> p) Reliability of variables</a></li>
<li class="chapter" data-level="11.1.17" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#q-the-nature-of-the-variables"><i class="fa fa-check"></i><b>11.1.17</b> q) The nature of the variables</a></li>
<li class="chapter" data-level="11.1.18" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#r-multiple-dependent-variables"><i class="fa fa-check"></i><b>11.1.18</b> r) Multiple dependent variables</a></li>
<li class="chapter" data-level="11.1.19" data-path="multiple-linear-regression-a-short-introduction.html"><a href="multiple-linear-regression-a-short-introduction.html#s-missing-variables"><i class="fa fa-check"></i><b>11.1.19</b> s) Missing variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="useful-r-codes.html"><a href="useful-r-codes.html"><i class="fa fa-check"></i><b>12</b> Useful R codes</a><ul>
<li class="chapter" data-level="12.1" data-path="useful-r-codes.html"><a href="useful-r-codes.html#more-on-the-apastyle-package"><i class="fa fa-check"></i><b>12.1</b> More on the apaStyle package</a></li>
<li class="chapter" data-level="12.2" data-path="useful-r-codes.html"><a href="useful-r-codes.html#a-useful-shiny-application"><i class="fa fa-check"></i><b>12.2</b> A useful shiny application</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An R Platform for Social Scientists</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multiple-linear-regression-a-short-introduction" class="section level1">
<h1><span class="header-section-number"> 11</span> Multiple Linear Regression, A Short Introduction</h1>
<p><em>Scientiﬁc development requires that knowledge be transferred reliably from one study to another and, as Galileo showed 350 years ago, such transference requires the precision and computational beneﬁts of a formal language.</em> <span class="citation">Pearl (<a href="#ref-Pearl09">2009</a>)</span></p>
<p>The <em>formal language</em> in the quote refers to <em>mathematical equations</em>. Galton for example, in late 1800s, used equations to describe the relationship between the weights of mother and daughter pea seeds. Galton’s work followed by Pearson’s contributions led to initial idea of regression.<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a></p>
<p>In the year 2016, the Web of Science reported that 60000+ abstracts of academic articles included the term “regression”. The literature is vast, oftentimes the regression is mentioned as the <em>workhorse</em>. It is extensively used by frequentist and Bayesian statisticians, and more generally by data scientists in hundreds of different disciplines. The explanation of the popularity of regression analysis is simple, unless they are simulated by a machine, connections between variables, whether observed or latent variables, in a data set requires more complex statistical solutions than are provided by correlation coefficients.</p>
<p>It is not feasible to cover regression in a book chapter. We briefly introduce basics of a relatively simple multiple regression model.</p>
<div id="matricies-and-least-square-estimation" class="section level2">
<h2><span class="header-section-number">11.1</span> Matricies and Least Square Estimation</h2>
<p>In a multiple regression framework, demonstrating process of model fitting based on matrices and least squares estimation should have at least two benefits; (a) a simple demystification of the procedure, (b) a workable and sensible foundation for readers with a desire to move further in the advanced topics. The following sections use two different data sets. The first data set includes only 12-cases to show calculations and is named the synthetic data. The second data is simulated with a larger sample size for illustrative purposes and is named the simulated data.</p>
<p>Consider a case in which data on three variables are collected and the researcher is interested in the relationship of one of the variables (i.e., the dependent variable) to the other two variables (i.e., the independent variables). Further, these three variables as continuous. The regression model in this scenario is</p>
<p><span class="math display">\[Y_i=\beta_0 + \beta_1X_{i1}+ \beta_2X_{i2}+ \epsilon_i\]</span> where <em>i</em> represents individuals i=1,…,n, Y is the dependent variable, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent variables, <span class="math inline">\(\beta\)</span>s are the regression coefficients and <span class="math inline">\(\epsilon\)</span> is the random error term(residuals) . This model can be presented in a matrix equations</p>
<p><span class="math display">\[Y=X\beta+\epsilon\]</span> In this more general form, all the independent variables are represented in the X matrix and the regression coefficients are represented by the <span class="math inline">\(\beta\)</span> matrix. Let us assume the researcher has the following data</p>
<table>
<thead>
<tr class="header">
<th align="left">id</th>
<th align="right">Y</th>
<th align="right">X1</th>
<th align="right">X2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">ind 1</td>
<td align="right">8</td>
<td align="right">0</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="left">ind 2</td>
<td align="right">4</td>
<td align="right">-2</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">ind 3</td>
<td align="right">6</td>
<td align="right">6</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="left">ind 4</td>
<td align="right">6</td>
<td align="right">-2</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">ind 5</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">ind 6</td>
<td align="right">9</td>
<td align="right">4</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="left">ind 7</td>
<td align="right">7</td>
<td align="right">3</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="left">ind 8</td>
<td align="right">-6</td>
<td align="right">-4</td>
<td align="right">-5</td>
</tr>
<tr class="odd">
<td align="left">ind 9</td>
<td align="right">-8</td>
<td align="right">-4</td>
<td align="right">-6</td>
</tr>
<tr class="even">
<td align="left">ind 10</td>
<td align="right">-1</td>
<td align="right">-3</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">ind 11</td>
<td align="right">0</td>
<td align="right">-2</td>
<td align="right">-2</td>
</tr>
<tr class="even">
<td align="left">ind 12</td>
<td align="right">5</td>
<td align="right">-1</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
<p>This synthetic data set has only 12 cases. The researcher can form 2 matrices and use these to calculate <span class="math inline">\(\hat\beta\)</span>, the estimate of <span class="math inline">\(\beta\)</span>.</p>
<p><span class="math display">\[Y=\begin{bmatrix}
8\\ 
4\\ 
6\\ 
6\\ 
5\\ 
9\\ 
7\\ 
-6\\ 
-8\\ 
-1\\ 
0\\ 
5
\end{bmatrix},X=\begin{bmatrix}
1 &amp; 0 &amp; 3\\ 
1 &amp; -2 &amp; 1\\ 
1 &amp; 6 &amp; 3\\ 
1 &amp; -3 &amp; 0\\ 
1 &amp; 5 &amp; 0\\ 
1 &amp; 4 &amp; 2\\ 
1 &amp; 3 &amp; 3\\ 
1 &amp; -4 &amp; -5\\ 
1 &amp; -4 &amp; -6\\ 
1 &amp; -3 &amp; 0\\ 
1 &amp; -2 &amp; -2\\ 
1 &amp; -1 &amp; 1
\end{bmatrix}\]</span></p>
<p>Using the least square procedure the <span class="math inline">\(\beta\)</span> coefficients can easily be estimated;</p>
<span class="math display" id="eq:lsebeta">\[\begin{equation} 
 \hat\beta=(X&#39;X)^{-1}X&#39;Y
  \tag{11.1}
\end{equation}\]</span>
<p>Let’s calculate this with R for the synthetic-data;</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Y=<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">8</span>,<span class="dv">4</span>,<span class="dv">6</span>,<span class="dv">6</span>,<span class="dv">5</span>,<span class="dv">9</span>,<span class="dv">7</span>,-<span class="dv">6</span>,-<span class="dv">8</span>,-<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">5</span>),<span class="dt">ncol=</span><span class="dv">1</span>)
X=<span class="kw">matrix</span>(<span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">12</span>),
               <span class="kw">c</span>(<span class="dv">0</span>,-<span class="dv">2</span>,<span class="dv">6</span>,-<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">3</span>,-<span class="dv">4</span>,-<span class="dv">4</span>,-<span class="dv">3</span>,-<span class="dv">2</span>,-<span class="dv">1</span>),
               <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">3</span>,-<span class="dv">5</span>,-<span class="dv">6</span>,<span class="dv">0</span>,-<span class="dv">2</span>,<span class="dv">1</span>)),<span class="dt">ncol=</span><span class="dv">3</span>)

<span class="kw">solve</span>(<span class="kw">t</span>(X)%*%X)%*%<span class="kw">t</span>(X)%*%Y
##       [,1]
## [1,] 2.917
## [2,] 0.199
## [3,] 1.552</code></pre></div>
<p>The regression equation is <span class="math display">\[\hat Y_i=2.9167 + 0.1989X_{i1} + 1.5519X_{i2} \]</span> where <span class="math inline">\(\hat Y_i\)</span> is the predicted value for the <span class="math inline">\(i^{th}\)</span> individual. Equation <a href="multiple-linear-regression-a-short-introduction.html#eq:lsebeta">(11.1)</a> was derived to minimize the error sum of squares: <span class="math inline">\(\sum_{i=1}^n(Y_i-\hat{Y_i})^2=Y&#39;Y-\beta&#39;X&#39;X\beta\)</span>.These estimates are Best Linear Unbiased Estimates.</p>
<p>Each independent variable has a mean of zero because they are mean-centered. Therefore, zero represents a score at the center of the distribution for both <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> and is therefore an interpretable score for both <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. When both predictors are zero (at their mean), the <span class="math inline">\((\hat{Y_i})\)</span> is 2.92. That is, for participants with independent variables scores equal to the mean on both independent variables the expected dependent variable score is 2.92. An increase in <span class="math inline">\(X_1\)</span> of 1 unit is predicted to correspond to an increase of 0.20 units in Y when the <span class="math inline">\(X_2\)</span> variable is held constant. Similarly, an increase in <span class="math inline">\(X_2\)</span> of 1 unit is predicted to correspond to an increase of 1.55 units in Y while controlling for <span class="math inline">\(X_1\)</span>. The term “controlling for” (“ceteris paribus”) is necessary to describe the effect of an independent variable in a multiple regression. The coefficients .20 and 1.55 would provide information about the association of the dependent and independent variables, if the researcher had substantial understanding of the unit of measurement for the independent variables, that is, the importance of a “1 unit” change in each variable.</p>
<div id="a-essentially-all-models-are-wrong-but-some-are-useful." class="section level3">
<h3><span class="header-section-number">11.1.1</span> a) “Essentially, all models are wrong, but some are useful.”</h3>
<p>This aphorism belongs to <span class="citation">Box and Draper (<a href="#ref-georgenorman87">1987</a>)</span>. The researcher should provide a convincing discussion about the relevance of the variables included in the regression model to the research questions addressed by the research. If there are important omitted variables, the beta coefficients are probably not valid. Hence the researcher is obligated to provide justifications on variable selections to claim usefulness of the results.</p>
<p>Consider the case below ;</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#omit X2 from the synthetic-data</span>
X2omitted=<span class="kw">matrix</span>(<span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">12</span>),<span class="kw">c</span>(<span class="dv">0</span>,-<span class="dv">2</span>,<span class="dv">6</span>,-<span class="dv">2</span>,<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">3</span>,-<span class="dv">4</span>,-<span class="dv">4</span>,-<span class="dv">3</span>,-<span class="dv">2</span>,-<span class="dv">1</span>)),<span class="dt">ncol=</span><span class="dv">2</span>)
<span class="kw">solve</span>(<span class="kw">t</span>(X2omitted)%*%X2omitted)%*%<span class="kw">t</span>(X2omitted)%*%Y
##      [,1]
## [1,] 2.92
## [2,] 1.09</code></pre></div>
<p>For our synthetic data, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> had a correlation of .68. If the researcher fails to include <span class="math inline">\(X_2\)</span> in the model, the coefficient for <span class="math inline">\(X_1\)</span> is estimated to be 1.09. This is a dramatic change from 0.20. Omitting predictors that are related to both the other predictors in the model and the dependent variable will cause the coefficients for the variables that have not been omitted to be misleading. Therefore an important part of the theoretical justification of a regression model is a discussion of variables that may have been omitted.<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a></p>
<p>In addition to omitted variable issue, the validity of the results from a regression model (the usefulness) is also directly related to the sampling process and appropriate reflection of this process in the model. For example, if sampling weights exist they should not be ignored in the analyses. <em>Sampling and regression</em> is beyond the scope of this chapter.</p>
</div>
<div id="b-strength-of-relationship-between-the-dependent-and-independent-variables" class="section level3">
<h3><span class="header-section-number">11.1.2</span> b) Strength of relationship between the dependent and independent variables</h3>
<p>The sum of squares for Y, which is also known as the total sum of squares, can be decomposed into two parts, <em>the model sum of squares</em>, which is also the sum of squares for the predicted values, and <em>the error sum of squares</em>. The ratio of the model sum of squares to <em>the total sum of squares</em>, is called the sample squared multiple correlation coefficient and symbolized as <span class="math inline">\(R^2\)</span>. The coefficient <span class="math inline">\(R^2\)</span> measures the strength of association between the dependent variable and the independent variables. Examine the R code below given for the synthetic data;</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># SS total</span>
n=<span class="kw">length</span>(Y)
TotalSS=<span class="kw">t</span>(Y)%*%Y-(n*<span class="kw">mean</span>(Y)^<span class="dv">2</span>)

<span class="co"># SS Model</span>
betahat=<span class="kw">solve</span>(<span class="kw">t</span>(X)%*%X)%*%<span class="kw">t</span>(X)%*%Y
ModelSS=<span class="kw">t</span>(betahat)%*%<span class="kw">t</span>(X)%*%Y-(n*<span class="kw">mean</span>(Y)^<span class="dv">2</span>)

ModelSS/TotalSS
##       [,1]
## [1,] 0.879</code></pre></div>
<p>Also known as <em>coefficient of determination</em>, <span class="math inline">\(R^2\)</span> is a biased estimator of the population squared multiple correlation coefficient. A more nearly unbiased estimate is the adjusted squared multiple correlation coefficient. One benefit of adjusted <span class="math inline">\(R^2\)</span> is computational simplicity. Examine the R code below given for the synthetic data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">
Rsquared=ModelSS/TotalSS
<span class="co">#sample size</span>
n=<span class="dv">12</span>

<span class="co">#the number of predictors</span>
p=<span class="dv">2</span>

<span class="co"># include intercept? 1 for yes, 0 for no</span>

int_inc=<span class="dv">1</span>


AdjustedRsquared=<span class="dv">1</span>-(<span class="dv">1</span>-Rsquared)*((n-int_inc)/(n-int_inc-p))
AdjustedRsquared
##       [,1]
## [1,] 0.852</code></pre></div>
<p><span class="math inline">\(R^2\)</span> and <span class="math inline">\(R^2_{Adj}\)</span> are useful coefficients; they provide information on how much of the variance is explained. Note that in this example <span class="math inline">\(R^2 = .879\)</span> and <span class="math inline">\(R^2_{Adj}= .852\)</span> are very similar. However if were <span class="math inline">\(R^2 = .25\)</span> then <span class="math inline">\(R^2_{Adj}\)</span> would be .08. When <span class="math inline">\(R^2\)</span> is 1, the model successfully explains 100% of the variance in Y and when <span class="math inline">\(R^2\)</span> is 0 the model does not explain any of the variance in Y. The coefficients <span class="math inline">\(R^2\)</span> and <span class="math inline">\(R^2_{Adj}\)</span> are also useful for comparing the strength of relationship for different set of predictors to predict a specific outcome. The interpretation of the <span class="math inline">\(R^2\)</span> is similar to the interpretation of a correlation coefficient. Depending on the context a small <span class="math inline">\(R^2\)</span> value might be regarded as substantial, or an <span class="math inline">\(R^2\)</span> value of .7 might be regarded as low.</p>
</div>
<div id="c-residuals-and-influential-data-points" class="section level3">
<h3><span class="header-section-number">11.1.3</span> c) Residuals and influential data points</h3>
<p>Residuals provide information for assessing potential problems with the model. Inspecting residuals can provide information about deviations from the assumed linearity of the relationships of the dependent variables to the independent variable. Inspecting the distributional properties of residuals is needed to provide evidence for the validity of statistical inference. For example, because the normality assumption is made when conducting significance tests and calculating confidence interval, residuals should follow a straight line on a Quantile-Quantile (QQ) plot. Examine the R code below given for the synthetic data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Predicted values</span>
Yhat=X%*%betahat
residuals=Y-Yhat
residuals
##          [,1]
##  [1,]  0.4276
##  [2,] -0.0708
##  [3,] -2.7658
##  [4,]  3.4811
##  [5,]  1.0888
##  [6,]  2.1839
##  [7,] -1.1691
##  [8,] -0.3615
##  [9,] -0.8096
## [10,] -3.3199
## [11,]  0.5850
## [12,]  0.7303
<span class="kw">qqnorm</span>(residuals)</code></pre></div>
<p><img src="SARP-EN_files/figure-html/unnamed-chunk-120-1.png" width="672" /></p>
<p>There are three common types of residuals;</p>
<p>• Unstandardized residuals, that is, <span class="math inline">\(Y_i-\hat{Y_i}\)</span>. Unstandardized residuals are on the same scale as Y.</p>
<p>• Standardized residuals: The residuals divided by the overall standard deviation of residuals; Standardized residuals are on a z-score scale (M = 0, SD = 1). When residuals are assumed to be normally distributed, it is common practice to identify outliers as Y values for which the absolute value of the standardized residual is larger than 2. However, it should be noted that this practice can be misleading because outliers can cause the regression coefficients to be poorly estimated and/or can increase the standard deviation of the residuals and both effects can cause poor outlier detection. In addition, if the residual are in fact normally distributed approximately %5 of the participants will have residuals beyone +/- 2.00. See <span class="citation">Wilcox (<a href="#ref-wilcox2012">2012</a>)</span> for more information about outlier detection.</p>
<p>• Studentized residual: A studentized residual is ratio of the unstandardized residual to the estimated standard error of the residual.</p>
<p>When investigating residuals , these three types of residuals generally lead to same conclusions. The standardized residual are forced have a z-scale, and thus, -2 and +2 are commonly pronounced cut offs. The studentized residuals are connected to the t distribution; <span class="math inline">\(t_{n-p&#39;-1}\)</span> where n is the sample size <span class="math inline">\(p&#39;\)</span> is the number of coefficients in the model (i.e intercept+two predictors =3). It is argued that when detecting outliers in residuals, investigating the studentized residuals is more convenient (<span class="citation">Rawlings, Pantula, and Dickey (<a href="#ref-Rawling98">1998</a>)</span>).</p>
<p>Scatter plots of residuals vs. predicted values can provide information about whether the assumed linear relationships between the independent variables and the dependent variable are adequate. Ideally the scatter plot should not show a detectable pattern. Here is a plot of studentized residuals vs fitted values, from a regression model fitted to simulated-data in which the linearity assumption is adequate. The simulated data have a sample size of 500 and two independent variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#simulate data</span>
<span class="kw">library</span>(mvtnorm)
sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="dt">ncol=</span><span class="dv">2</span>)
xx &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dt">n=</span><span class="dv">500</span>, <span class="dt">mean=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">sigma=</span>sigma)
yy=<span class="dv">5</span>+xx[,<span class="dv">1</span>]*<span class="dv">2</span>+xx[,<span class="dv">2</span>]*-<span class="dv">3</span>+<span class="kw">rnorm</span>(<span class="dv">500</span>,<span class="dv">0</span>,<span class="fl">1.5</span>)
model=<span class="kw">lm</span>(yy~xx[,<span class="dv">1</span>]+xx[,<span class="dv">2</span>])
errors=<span class="kw">rstudent</span>(model)
predicted=<span class="kw">predict</span>(model)

<span class="co">#Standardized Residuals vs Yhat</span>
<span class="kw">library</span>(ggplot2)
plotdata=<span class="kw">data.frame</span>(errors,predicted)
<span class="kw">ggplot</span>(plotdata, <span class="kw">aes</span>(<span class="dt">x =</span> predicted, <span class="dt">y =</span> errors)) +
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_hline</span>(<span class="dt">yintercept=</span><span class="dv">0</span>)+<span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Studentized residuals&quot;</span>)+
<span class="st">  </span><span class="kw">theme_bw</span>()+<span class="kw">stat_smooth</span>()</code></pre></div>
<p><img src="SARP-EN_files/figure-html/unnamed-chunk-121-1.png" width="672" /></p>
<p>The blue line above, which is determined independently from the regression model, should be compared to the horizontal line at 0. The more similar the two lines, the less likely the linearity assumption is violated.</p>
<p>Here is a plot, studentized residuals vs fitted values, from a mis-specified regression model on a simulated-data. The sample size is 100, there are two independent variables, and the relationship of Y and X2 is quadratic.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#simulate data</span>
<span class="kw">library</span>(mvtnorm)
sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="dt">ncol=</span><span class="dv">2</span>)
xx &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dt">n=</span><span class="dv">100</span>, <span class="dt">mean=</span><span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">10</span>), <span class="dt">sigma=</span>sigma)
yy=<span class="dv">150</span>+(xx[,<span class="dv">1</span>]*<span class="dv">4</span>)+(xx[,<span class="dv">2</span>]*-<span class="dv">3</span>)+(xx[,<span class="dv">2</span>]^<span class="dv">2</span>*<span class="fl">1.2</span>)+<span class="kw">rnorm</span>(<span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">3</span>)
model=<span class="kw">lm</span>(yy~xx[,<span class="dv">1</span>]+xx[,<span class="dv">2</span>])
errors=<span class="kw">rstudent</span>(model)
predicted=<span class="kw">predict</span>(model)

<span class="co">#Studentized Residuals vs Yhat</span>
<span class="kw">library</span>(ggplot2)
plotdata=<span class="kw">data.frame</span>(errors,predicted)
<span class="kw">ggplot</span>(plotdata, <span class="kw">aes</span>(<span class="dt">x =</span> predicted, <span class="dt">y =</span> errors)) +
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_hline</span>(<span class="dt">yintercept=</span><span class="dv">0</span>)+<span class="kw">ylab</span>(<span class="st">&quot;Studentized residuals&quot;</span>)+
<span class="st">  </span><span class="kw">theme_bw</span>()+<span class="kw">stat_smooth</span>()</code></pre></div>
<p><img src="SARP-EN_files/figure-html/unnamed-chunk-122-1.png" width="672" /></p>
<p>There is a pattern indicating that the model is omitting a quadratic association.However, this graph does not inform about the source of the quadratic association, see non-linearity section below.</p>
<p>Unusual residuals should be inspected. Even when the residuals are substantially normally distributed and there is substantially no-pattern for the residual vs predicted value plot, there might be unusual residuals. Deciding whether a residual is unusual or not (e.g 3,4 or 5 standard deviation above), and more importantly whether to keep the observation in the data set or not requires justifications. Examine the code below to simulate data and examine the studentized residuals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#simulate data</span>
<span class="kw">set.seed</span>(<span class="dv">04022017</span>)
<span class="kw">library</span>(mvtnorm)
sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="dt">ncol=</span><span class="dv">2</span>)
xx &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dt">n=</span><span class="dv">100</span>, <span class="dt">mean=</span><span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">10</span>), <span class="dt">sigma=</span>sigma)
yy=(xx[,<span class="dv">1</span>]*<span class="dv">4</span>)+(xx[,<span class="dv">2</span>]*-<span class="dv">3</span>)+<span class="kw">rnorm</span>(<span class="dv">100</span>,<span class="dv">0</span>,<span class="dv">3</span>)
tempdata=<span class="kw">data.frame</span>(yy,xx,<span class="dt">id=</span><span class="dv">1</span>:<span class="dv">100</span>)
model=<span class="kw">lm</span>(yy~X1+X2,<span class="dt">data=</span>tempdata)
tempdata$SUTresiduals=<span class="kw">rstudent</span>(model)
<span class="co"># how many of the residuals are larger than a critical value?</span>
<span class="co"># lets use alpha=.05</span>
<span class="kw">sum</span>(<span class="kw">abs</span>(tempdata$SUTresiduals)&gt;<span class="kw">qt</span>(<span class="kw">c</span>(.<span class="dv">975</span>), <span class="dt">df=</span><span class="dv">100-3-1</span>))
## [1] 8

<span class="co">#which observations?</span>
tempdata[<span class="kw">which</span>(<span class="kw">abs</span>(tempdata$SUTresiduals)&gt;<span class="kw">qt</span>(<span class="kw">c</span>(.<span class="dv">975</span>), <span class="dt">df=</span><span class="dv">100-3-1</span>)),]
##       yy    X1    X2 id SUTresiduals
## 13 21.39 11.49 10.29 13         2.02
## 32  8.85 11.96 10.65 32        -2.20
## 43 15.80 11.14  7.56 43        -1.99
## 50  9.21  8.00 10.21 50         2.53
## 51 19.96 10.11  8.97 51         2.02
## 68 25.33 10.96  8.33 68         2.04
## 84  2.03  7.94  7.84 84        -2.03
## 91  5.51 10.74 10.25 91        -2.10</code></pre></div>
<p>Assume we justified the use of <span class="math inline">\(t_{.975,96}\)</span> as the critical value, in which alpha=.05. We should expect approximately <span class="math inline">\(n * .05\)</span> (in our case 100*.05=5) cases larger than the critical value. In this particular case, even though 8 cases were identified, none of them seems unusual.</p>
<p>If the researcher detects an abnormality and further, if the researcher decides to remove the observation from the data, it should be done one observation at a time. The justification of removing a data point should be given clearly. A better alternative, on the other hand, may be to use an estimation method that is robust to outlying data points.</p>
<p>R program is convenient for investigating influential data points. Examine <em>?influence.measures</em> below for the simulated data set;</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">influence.measures</span>(model))
## Potentially influential observations of
##   lm(formula = yy ~ X1 + X2, data = tempdata) :
## 
##     dfb.1_ dfb.X1 dfb.X2 dffit cov.r   cook.d hat  
## 12   0.08  -0.02  -0.08  -0.10  1.12_*  0.00   0.08
## 33   0.09  -0.03  -0.07  -0.11  1.11_*  0.00   0.07
## 41  -0.01  -0.03   0.03  -0.04  1.10_*  0.00   0.06
## 42   0.05  -0.12   0.07   0.13  1.11_*  0.01   0.07
## 50   0.20  -0.40   0.21   0.47  0.88_*  0.07   0.03
## 64  -0.03   0.03   0.00   0.04  1.10_*  0.00   0.06
## 100  0.01   0.13  -0.15  -0.18  1.10_*  0.01   0.07</code></pre></div>
<p>This output reports 5 different measures.</p>
<p>In this example, cases 12, 33, 41, 42, 50, 64 and 100 are reported to be <em>potentially</em> influential. As they highlighted by an asterisk, they labeled as potential using the covariance ratio criteria (cov.r). This value reports the impact of an observation on the sampling variances of the regression coefficients. Values larger than <span class="math inline">\(1+(3p&#39;/n)\)</span> and lower than <span class="math inline">\(1-(3p&#39;/n)\)</span> are labeled as influential, in our case, n=100 and p’=3, hence the cut offs are 1.09 and .91.</p>
<p>The Dfb (DFBETAS) for each predictor reports how much the coefficient for the predictor changes when the case is removed. It is the difference between the two coefficients divided by an estimate of the standard error of the new coefficient and therefore is on the scale of a t statistic. R places an asterisk if the value is larger than <span class="math inline">\(2/\sqrt(n)\)</span>. For this specific illustration the cut off value is <span class="math inline">\(2/\sqrt(100)=.2\)</span>.</p>
<p>The dffit reports the change in the predicted value for the <span class="math inline">\(i^{th}\)</span> case when the <span class="math inline">\(i^{th}\)</span> case is removed from the data. The criterion for identifying potentially influential data points is <span class="math inline">\(2*\sqrt{\frac{p&#39;}{n}}\)</span>.</p>
<p>Cook’s distance (cook.d) measures the influence of a particular case on all of the estimated coefficients and values larger than <span class="math inline">\(F_{.5,p&#39;,n-p&#39;}\)</span> are highlighted. Cook’s distance also measures influence of omitting a particular case of the predicted values for all of the remaining cases.</p>
<p>Leverage Values (Hat Diag) measure the distance of an observation compared to other independent variables. Values larger than <span class="math inline">\(2p’/n\)</span> are considered to identify potentially influential data points.</p>
<p>It is researcher’s responsibility to examine any potentially influential data points.</p>
</div>
<div id="d-equal-variance-assumption" class="section level3">
<h3><span class="header-section-number">11.1.4</span> d) <em>Equal variance assumption</em></h3>
<p>The standard errors of the of the coefficients are calculated as the square roots of the diagonal elements of <span class="math inline">\(\hat\sigma^2(X′X)^{-1}\)</span>, where <span class="math inline">\(\hat\sigma^2\)</span> is the variance of the residuals. Examine the code below given for the synthetic data set:</p>
<p>When using the OLS with an assumption of normally distributed Y variable, the distribution of <span class="math inline">\(\beta\)</span> can be obtained. Examine the code below given for the synthetic-data set;</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Residuals</span>
s2 &lt;-<span class="st"> </span>(<span class="kw">t</span>(residuals) %*%<span class="st"> </span>residuals)/(<span class="kw">nrow</span>(Y)-<span class="kw">nrow</span>(betahat))  
Var_betahat &lt;-<span class="st"> </span>s2[<span class="dv">1</span>,<span class="dv">1</span>]*<span class="kw">solve</span>(<span class="kw">t</span>(X)%*%X)  </code></pre></div>
<p>The equation <span class="math inline">\(\sigma^2 (X&#39;X)^{-1}\)</span> is valid under the assumption of homogeneity, that is, observations on the Y variable have a common variance controlling for the independent variables. In other words, every observation of Y has the same amount of information (<span class="citation">Rawlings, Pantula, and Dickey (<a href="#ref-Rawling98">1998</a>)</span>). With this assumption, regression coefficients are selected to minimize <span class="math inline">\(\sum_{i=1}^n(Y_i-\hat{Y_i})^2\)</span>. In this expression equal weights are given to the residuals for every case.. If homogeneity is questionable the estimator can be modified to allow for unequal weights or replaced. Alternatively the Y variable can be transformed or the estimator of the standard error can be modified (see package ‘sandwich’ <span class="citation">Lumley and Zeileis (<a href="#ref-R-sandwich">2015</a>)</span>). Otherwise, the standard error of <span class="math inline">\(\hat\beta\)</span> could be underestimated or overestimated. Underestimation results in Type I error rates that are larger than the alpha level used in hypothesis tests and confidence intervals and over estimation results in reduced statistical power. It is common practice to plot residuals against the predicted values to study heterogeneity. Examine the code below to simulate data with unequal variance and examine the studentized residuals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#simulate data</span>
<span class="kw">set.seed</span>(<span class="dv">03032017</span>)
<span class="kw">library</span>(mvtnorm)
sigma &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,.<span class="dv">7</span>,.<span class="dv">7</span>,<span class="dv">1</span>), <span class="dt">ncol=</span><span class="dv">2</span>)
xx &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dt">n=</span><span class="dv">100</span>, <span class="dt">mean=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">sigma=</span>sigma)
<span class="co">#heteroscedasticity function</span>
hts=function(v1,v2){<span class="dv">2</span><span class="fl">+.5</span>*v1<span class="fl">+.5</span>*v2}
yy=<span class="dv">5</span>+xx[,<span class="dv">1</span>]*<span class="dv">5</span>+xx[,<span class="dv">2</span>]*<span class="dv">5</span>+<span class="kw">rnorm</span>(<span class="dv">100</span>,<span class="dv">0</span>,<span class="kw">hts</span>(xx[,<span class="dv">1</span>],xx[,<span class="dv">2</span>]))
model=<span class="kw">lm</span>(yy~xx[,<span class="dv">1</span>]+xx[,<span class="dv">2</span>])
<span class="co">#summary(model)</span>
errors=<span class="kw">rstudent</span>(model)
predicted=<span class="kw">predict</span>(model)

<span class="co">#Studentized Residuals vs Yhat</span>
<span class="kw">library</span>(ggplot2)
plotdata=<span class="kw">data.frame</span>(errors,predicted)
<span class="kw">ggplot</span>(plotdata, <span class="kw">aes</span>(<span class="dt">x =</span> predicted, <span class="dt">y =</span> errors)) +
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_hline</span>(<span class="dt">yintercept=</span><span class="dv">0</span>)+<span class="kw">ylab</span>(<span class="st">&quot;Studentized residuals&quot;</span>)+
<span class="st">  </span><span class="kw">geom_segment</span>(<span class="dt">mapping=</span><span class="kw">aes</span>(<span class="dt">xend =</span> predicted, <span class="dt">yend =</span> <span class="dv">0</span>)) +
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="SARP-EN_files/figure-html/unnamed-chunk-126-1.png" width="672" /></p>
<p>The variance with smaller <span class="math inline">\(\hat Y\)</span> values are smaller. Below is a graph for a regression model on a simulated data with equal variance.</p>
<p><img src="SARP-EN_files/figure-html/unnamed-chunk-127-1.png" width="672" /></p>
</div>
<div id="e-hypothesis-testing" class="section level3">
<h3><span class="header-section-number">11.1.5</span> e) Hypothesis testing</h3>
<p>The F test is used within a multiple regression framework to test <span class="math inline">\(H_0: \beta_1=...=\beta_p=0\)</span> , a hypothesis stating that the p regression coefficients are all equal to zero in the population. The alternative hypothesis states that at least one coefficient is not zero. The null hypothesis can be tested using the statistic <span class="math inline">\(MS_{regression}/MS_{residual}\)</span>. This statistic follows an F distribution with <span class="math inline">\(p\)</span> and <span class="math inline">\(n-p&#39;\)</span> degrees of freedom. As mentioned earlier, p is the number of predictors and p’ is the number of coefficients (p’=p if there is no intercept). Examine the code below given for the synthetic data, setting Type I error rate = .05;</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Model SS and Total SS calculated before</span>
dfREG=<span class="dv">2</span>  <span class="co">#(p=2, predictors X1 and X2)</span>
dfRES=<span class="dv">9</span>  <span class="co">#(n-p&#39;, 12-3)</span>
MSreg=ModelSS/dfREG
MSres=(TotalSS-ModelSS)/dfRES

MSreg/MSres
##      [,1]
## [1,] 32.8

<span class="co">#critical F</span>
<span class="kw">qf</span>(.<span class="dv">95</span>,dfREG,dfRES)
## [1] 4.26
<span class="dv">1</span>-<span class="kw">pf</span>(MSreg/MSres,dfREG,dfRES)
##          [,1]
## [1,] 7.39e-05</code></pre></div>
<p>The t-test is used for investigating <span class="math inline">\(H_0:\beta_{X}=\beta_{hyp}\)</span> vs <span class="math inline">\(H_1:\beta_{X}\neq\beta_{hyp}\)</span>. Most commonly <span class="math inline">\(\beta_{hyp}=0\)</span></p>
<p>The statistic <span class="math inline">\((b_X-\beta_{hyp})/SE(b_X)\)</span> follows a t-distribution with N-p’ degrees of freedom.Examine the code below given for the synthetic-data;</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># test if the coefficient for X2 is different than 0</span>
Bhyp=<span class="dv">0</span>  <span class="co">#hypothesized value</span>

<span class="co"># estimated coefficient for X2 (see betahat calculated before)</span>
bx2=betahat[<span class="dv">3</span>]  

<span class="co"># estimated SE for X2 (see var_betahat calculated before)</span>
se_bx2=<span class="kw">sqrt</span>(Var_betahat[<span class="dv">3</span>,<span class="dv">3</span>])

<span class="co">#t statistic</span>
(bx2-Bhyp)/se_bx2
## [1] 5.33

<span class="co"># t critic</span>
<span class="kw">qt</span>(.<span class="dv">975</span>,<span class="dv">9</span>)
## [1] 2.26

<span class="co">#p value</span>
<span class="dv">2</span>*(<span class="kw">pt</span>(-<span class="kw">abs</span>((bx2-Bhyp)/se_bx2),<span class="dv">9</span>))
## [1] 0.000478</code></pre></div>
</div>
<div id="f-variable-selection" class="section level3">
<h3><span class="header-section-number">11.1.6</span> f) Variable Selection</h3>
<p>Broadly speaking there are two situations in which multiple regression is used to analyze data.</p>
<p>The first is illustrated by the following example. A social science researcher conducts an extensive literature review, identifies all independent variables relevant to the research questions, collects the data, estimates a model in which all independent variables are included and reports results for this model.</p>
<p>The second is illustrated by an example in which the researcher has data on a very large set of variables and does not know prior to analyzing the data which variables will be included in the final model that will be reported. This might happen because the researcher is working in a relatively new research area and collects data on a wide variety of variables or is conducting a secondary data analysis of a data set with a wide variety of predictors. In either case the researcher may want to begin by conducting variable selection that is using statistical results to select the best subset of many independent variables. There are several approaches to select the best subset of predictors. For example, stepwise regression, backward selection or forward selection is covered in many sources. However, in our experience, when applied to the same data set these three approaches are likely to give different answers.</p>
<p>A convenient approach with R is to study all possible regressions.For introductory purposes, examine the code below given for a simulated data set;</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#simulate data</span>
<span class="kw">set.seed</span>(<span class="dv">02082017</span>)
<span class="kw">library</span>(mvtnorm)
sigma=<span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">5.899559</span>,<span class="fl">4.277045</span>,<span class="fl">3.906341</span>,
               <span class="fl">4.277045</span>,<span class="fl">5.817412</span>,<span class="fl">3.654419</span>,
               <span class="fl">3.906341</span>,<span class="fl">3.654419</span>,<span class="fl">5.642258</span>),<span class="dt">ncol=</span><span class="dv">3</span>)
xx &lt;-<span class="st"> </span><span class="kw">rmvnorm</span>(<span class="dt">n=</span><span class="dv">200</span>, <span class="dt">mean=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">sigma=</span>sigma)
yy=<span class="dv">5</span>+xx[,<span class="dv">1</span>]+xx[,<span class="dv">2</span>]*<span class="fl">1.5</span>+xx[,<span class="dv">3</span>]*<span class="dv">2</span>+<span class="kw">rnorm</span>(<span class="dv">200</span>,<span class="dv">0</span>,<span class="dv">3</span>)
simdata=<span class="kw">data.frame</span>(yy,xx,<span class="dt">id=</span><span class="dv">1</span>:<span class="dv">200</span>)

<span class="kw">library</span>(leaps)
formula &lt;-<span class="st"> </span><span class="kw">formula</span>(<span class="kw">paste</span>(<span class="st">&quot;yy ~ &quot;</span>, 
     <span class="kw">paste</span>(<span class="kw">names</span>(simdata[<span class="dv">2</span>:<span class="dv">4</span>]), <span class="dt">collapse=</span><span class="st">&quot; + &quot;</span>)))
allpossreg &lt;-<span class="st"> </span><span class="kw">regsubsets</span>(formula,<span class="dt">nbest=</span><span class="dv">3</span>,<span class="dt">data=</span>simdata)
aprout &lt;-<span class="st"> </span><span class="kw">summary</span>(allpossreg)

<span class="co">#this functions reports more than R-squared and adjusted R-squared</span>
<span class="co">#examine str(aprout)</span>


APRtable=<span class="kw">with</span>(aprout,<span class="kw">round</span>(<span class="kw">cbind</span>(which,rsq,adjr2),<span class="dv">3</span>))
APRtable=<span class="kw">data.frame</span>(APRtable,<span class="dt">check.rows =</span> F,<span class="dt">row.names =</span> <span class="ot">NULL</span>)
APRtable$ppri=<span class="kw">rowSums</span>(APRtable[,<span class="dv">1</span>:<span class="dv">4</span>])
<span class="kw">kable</span>(APRtable)</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">X.Intercept.</th>
<th align="right">X1</th>
<th align="right">X2</th>
<th align="right">X3</th>
<th align="right">rsq</th>
<th align="right">adjr2</th>
<th align="right">ppri</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0.753</td>
<td align="right">0.751</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.696</td>
<td align="right">0.695</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0.630</td>
<td align="right">0.629</td>
<td align="right">2</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0.871</td>
<td align="right">0.870</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0.811</td>
<td align="right">0.809</td>
<td align="right">3</td>
</tr>
<tr class="even">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0.808</td>
<td align="right">0.806</td>
<td align="right">3</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="right">0.890</td>
<td align="right">0.888</td>
<td align="right">4</td>
</tr>
</tbody>
</table>
<p>This table reports that intercept and <span class="math inline">\(X_2\)</span> only model results in an <span class="math inline">\(R^2\)</span> value of .753. When all predictors included, the <span class="math inline">\(R^2\)</span> reaches to .890; however, excluding the X1 from the full model reduced the <span class="math inline">\(R^2\)</span> only by .019. Below is a graphical decpiction.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(ggplot2)
<span class="kw">ggplot</span>(APRtable, <span class="kw">aes</span>(<span class="dt">x=</span>ppri<span class="dv">-1</span>, <span class="dt">y=</span>rsq)) +
<span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">shape=</span><span class="dv">1</span>,<span class="dt">size=</span><span class="dv">3</span>)+<span class="st">   </span>
<span class="st">    </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dt">by =</span> <span class="fl">0.05</span>)) +
<span class="st">    </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dt">by =</span> <span class="dv">1</span>))+
<span class="st">    </span><span class="kw">theme_bw</span>()+<span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;R-squared&quot;</span>)+<span class="st"> </span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">14</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))

<span class="kw">ggplot</span>(APRtable, <span class="kw">aes</span>(<span class="dt">x=</span>ppri<span class="dv">-1</span>, <span class="dt">y=</span>adjr2)) +
<span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">shape=</span><span class="dv">1</span>,<span class="dt">size=</span><span class="dv">3</span>)+<span class="st">   </span>
<span class="st">    </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dt">by =</span> <span class="fl">0.05</span>)) +
<span class="st">    </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dt">by =</span> <span class="dv">1</span>))+
<span class="st">      </span><span class="kw">theme_bw</span>()+<span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Adjusted R-squared&quot;</span>)+<span class="st"> </span>
<span class="st">    </span><span class="kw">theme</span>(<span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">14</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-131"></span>
<img src="SARP-EN_files/figure-html/unnamed-chunk-131-1.png" alt="All Possible Regressions" width="50%" /><img src="SARP-EN_files/figure-html/unnamed-chunk-131-2.png" alt="All Possible Regressions" width="50%" />
<p class="caption">
Figure 11.1: All Possible Regressions
</p>
</div>
</div>
<div id="g-colliniearity" class="section level3">
<h3><span class="header-section-number">11.1.7</span> g) Colliniearity</h3>
<p>Collinearity is the degree to which the predictors are correlated among themselves. The correlation between predictors is a concern in regression because the standard errors of the coefficients increase as collinearity increase and therefore collinearity hides the individual contribution of each predictor in the regression equation.</p>
<p>As an illustration suppose there are two independent variables with r = .9. You MIGHT have two types of problems: The regression coefficients become unstable (i.e. they would vary a great deal across different samples obtained from the same population).</p>
<p>You may obtain a statistically significant <span class="math inline">\(R^2\)</span> but not statistically significant regression coefficients.</p>
<p>Variance inflation factor (VIF) is helpful to detect collinearity in regard to a particular independent variables and can be applied in models two or more independent variables. The formula is <span class="math inline">\(VIF_x=\frac{1}{1-R^2_X}\)</span> where <span class="math inline">\(R^2_X\)</span> is the <span class="math inline">\(R^2\)</span> when the predictor is regressed on the remaining independent variables. Large VIF values are indicator of possible multicollinearity. Commonly pronounced cut off values are 4 and 10, however VIF values are indirectly affected by sample size and variance (<span class="citation">Obrien (<a href="#ref-obrien09">2007</a>)</span>). When large VIF values are detected, the researcher should examine the problem. It might be justifiable to (a) leave out one of the highly correlated predictor, (b) combine the two highly correlated variable. The decision should be made cautiously given that the possible solution might be more problematic than a large VIF value, see <span class="citation">Obrien (<a href="#ref-obrien09">2007</a>)</span>. Examine the code below given for a simulated data set;</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#check correlations among predictors</span>
<span class="kw">cor</span>(simdata[,<span class="dv">2</span>:<span class="dv">4</span>])
##      X1    X2    X3
## X1 1.00 0.730 0.640
## X2 0.73 1.000 0.666
## X3 0.64 0.666 1.000

<span class="co">#the largest correlation is.73</span>
<span class="co">#no multicollinearity expected</span>

<span class="kw">library</span>(car)
<span class="kw">vif</span>(<span class="kw">lm</span>(yy~X1+X2+X3,<span class="dt">data=</span>simdata))
##   X1   X2   X3 
## 2.36 2.50 1.98
<span class="co"># no problematic VIF values</span></code></pre></div>
</div>
<div id="h-non-linearity" class="section level3">
<h3><span class="header-section-number">11.1.8</span> h) Non-linearity</h3>
<p>In the presence of a non-linear relation between the dependent variable and any given independent variable, ignoring non-linearity is simply a validity concern due to the omitted variable issue. Examining the residuals is helpful to detect non-linearity. Residuals should be plotted against predicted and independent variables. A common practice is to include higher order variables in the model, for example, if the plot indicates a non linear pattern for <span class="math inline">\(X_k\)</span> against residuals, <span class="math inline">\(X_k^2\)</span> might be needed in the model. The type of the non-linearity , such as quadratic, cubic or quartic should be treated accordingly. <span class="citation">Gelman and Hill (<a href="#ref-gelman07">2007</a>)</span> , commenting on age variable when the age and dependent variable are not linearly associated, prefers treating the variable as a categorical predictor. Alternatively transformations of the dependent or independent variable may be employed.</p>
</div>
<div id="i-correlated-errors-and-nonindependent-errors" class="section level3">
<h3><span class="header-section-number">11.1.9</span> i) Correlated errors and nonindependent errors</h3>
<p>Errors should not be correlated or more broadly should be independent. When such dependency is not addressed, regression results are invalid. This topic ,however, is well beyond the scope of this introductory material. Correlated errors are likely to distort the standard errors for the beta coefficients. This is not desired. In social sciences, correlated errors might be present when measurements are repeated. Multilevel models and latent growth models has been developed to address appropriate modeling of repeated measure designs. Nesting of participants in subgoups is another common source of non-independent errors in social sciences. Multilevel models are one popular solution to model clustered data.</p>
</div>
<div id="j-centering-and-scaling" class="section level3">
<h3><span class="header-section-number">11.1.10</span> j) Centering and Scaling</h3>
<p>Consider an example in which mother’s age at the date of her child’s birth (maternal age) is used to predict IQ at age 10. The intercept estimates average IQ for children whose mother’s maternal age was zero and cannot be meaningfully interpreted. Centering maternal age around its mean results in an intercept which estimates average IQ for children whose mother’s maternal age was at the mean of the sample and can be meaningfully interpreted. Or consider predicting absences from work from an anxiety measure. A score of zero is possible score on the anxiety measure, but does not occur in the sample. The intercept estimates average absences for employees whose anxiety level is outside the range of the data and therefore represents an extrapolation for the data. Centering around the mean for the sample solves this problem. Another approach would be to center around an anxiety score that is in the range of the data and considered high. Or consider a study of income and an index of health. Income is on a scale in which a change of 1 represents a change of 1 dollar in income. The regression coefficient is .001, which represents a trivial change in the index. Dividing X by 1000 so that a change of 1 represents a change of 1000 dollars in income results in a regression coefficient of 1, which is a small but not trivial change in the index may make the results easier to think about.</p>
</div>
<div id="k-standardized-coefficients" class="section level3">
<h3><span class="header-section-number">11.1.11</span> k) Standardized coefficients</h3>
<p>A related topic to linear transformations is to use a z-score for the continious predictors by substracting the mean and dividing by the standard deviation. Depending on the nature of the variable, using the z scores might ease the communication between researchers. Here are interpretation examples; Raw scores: An increase in anxiety of 1 unit is predicted to correspond to an increase of 3 units in achievement, holding the remaining predictors constant. z-scores: An increase in motivation of 1 standard deviation is predicted to correspond to an increase of 0.25 standard deviations in achievement, holding the the remaining predictors constant.</p>
</div>
<div id="l-interactions" class="section level3">
<h3><span class="header-section-number">11.1.12</span> l) Interactions</h3>
<p>We covered the basic idea of interaction in our ANOVA section. Ignoring an interaction is an omitted variable problem because an interaction affects the interpretation of main effects. For example, suppose a researcher investigates the relationship between mathematics achievement at the end of the school year (<span class="math inline">\(Y\)</span>), effort measured by voluntary homework completed and submitted during the year (<span class="math inline">\(X_1\)</span>), and mathematics achievement at the end of the preceding year (<span class="math inline">\(X_2\)</span>). Using the model <span class="math inline">\(Y_i=\beta_0+\beta_1X_{i1}+\beta_2X_{i2}+\epsilon_i\)</span> assumes that the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span> does not depend on <span class="math inline">\(X_2\)</span> and will be misleading if the assumption is false. A common model used to investigate interactions is ;</p>
<span class="math display" id="eq:olsinteract">\[\begin{equation} 
 Y_i=\beta_0+\beta_1 X_{i1}+\beta_2 X_{i2}+\beta_3 X_{i1} X_{i2}+ \epsilon_i
  \tag{11.2}
\end{equation}\]</span>
<p>The slope of the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span>, for example, is <span class="math inline">\(\beta_1+\beta_2 X_{i2}\)</span> indicating that the relationship depends on <span class="math inline">\(X_2\)</span>. Similarly the slope relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_2\)</span> is <span class="math inline">\(\beta_2+\beta_1 X_{i1}\)</span>. Consideration of <span class="math inline">\(\beta_1+\beta_2 X_{i2}\)</span> shows that <span class="math inline">\(\beta_1\)</span> is the slope of the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span> when <span class="math inline">\(X_2 = 0\)</span> and therefore <span class="math inline">\(\beta_1\)</span> cannot be meaningfully interpreted if <span class="math inline">\(X_2 = 0\)</span> is not a meaningful score or is outside the range of the data. This problem can be addresses by centering <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> around their respective means. It should be noted that the model in Equation <a href="multiple-linear-regression-a-short-introduction.html#eq:olsinteract">(11.2)</a> assumes that the interaction can be accurately modeled by including <span class="math inline">\(\beta_2 X_{i1}X_{i2}\)</span> in the model. This assumes the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1\)</span> is linear when is <span class="math inline">\(X_2\)</span> controlled. Violations of assumption of the model in Equation <a href="multiple-linear-regression-a-short-introduction.html#eq:olsinteract">(11.2)</a> should be investigated.R can be helpful in interpreting interactions by 3-dimension graphs. Examine the code below given for a simulated data set to highlight the use of R package <em>visreg</em> (<span class="citation">Breheny and Burchett (<a href="#ref-R-visreg">2016</a>)</span>)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## manipulate simdata
## Yvar: dependent variable on no interaction
simdata$Yvar=<span class="dv">3</span>+simdata$X1*<span class="dv">2</span>+simdata$X2*<span class="dv">3</span>+<span class="kw">rnorm</span>(<span class="kw">nrow</span>(simdata),<span class="dv">0</span>,<span class="dv">5</span>)

<span class="kw">library</span>(visreg)

model=<span class="kw">lm</span>(Yvar~X1+X2+X1*X2,<span class="dt">data=</span>simdata)
<span class="kw">visreg2d</span>(model, <span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>, <span class="dt">plot.type=</span><span class="st">&quot;persp&quot;</span>)</code></pre></div>
<p><img src="SARP-EN_files/figure-html/unnamed-chunk-133-1.png" width="672" /></p>
<p>The surface is flat indicating no interaction.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## manipulate simdata
## Yvarint: dependent variable on a interaction
simdata$Yvarint=<span class="dv">3</span>+simdata$X1*<span class="dv">1</span>+simdata$X2*<span class="dv">2</span>+simdata$X1*simdata$X2*<span class="fl">1.5</span>+<span class="kw">rnorm</span>(<span class="kw">nrow</span>(simdata),<span class="dv">0</span>,<span class="dv">5</span>)

<span class="kw">library</span>(visreg)

model2=<span class="kw">lm</span>(Yvarint~X1+X2+X1*X2,<span class="dt">data=</span>simdata)
<span class="kw">visreg2d</span>(model2, <span class="st">&quot;X1&quot;</span>, <span class="st">&quot;X2&quot;</span>, <span class="dt">plot.type=</span><span class="st">&quot;persp&quot;</span>)</code></pre></div>
<p><img src="SARP-EN_files/figure-html/unnamed-chunk-134-1.png" width="672" /></p>
<p>The surface is no longer flat in the presence of an interaction.</p>
</div>
<div id="m-estimators" class="section level3">
<h3><span class="header-section-number">11.1.13</span> m) Estimators</h3>
<p>To be added</p>
</div>
<div id="n-robust-regression" class="section level3">
<h3><span class="header-section-number">11.1.14</span> n) Robust Regression</h3>
<p>To be added</p>
</div>
<div id="o-sample-size-and-statistical-power" class="section level3">
<h3><span class="header-section-number">11.1.15</span> o) Sample size and statistical power</h3>
<p>To be added</p>
</div>
<div id="p-reliability-of-variables" class="section level3">
<h3><span class="header-section-number">11.1.16</span> p) Reliability of variables</h3>
<p>To be added</p>
</div>
<div id="q-the-nature-of-the-variables" class="section level3">
<h3><span class="header-section-number">11.1.17</span> q) The nature of the variables</h3>
<p>To be added</p>
</div>
<div id="r-multiple-dependent-variables" class="section level3">
<h3><span class="header-section-number">11.1.18</span> r) Multiple dependent variables</h3>
<p>To be added</p>
</div>
<div id="s-missing-variables" class="section level3">
<h3><span class="header-section-number">11.1.19</span> s) Missing variables</h3>
<p>To be added</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Pearl09">
<p>Pearl, Judea. 2009. <em>Causality: Models, Reasoning, and Inference</em>. 2nd ed. Cambridge;New York: Cambridge University Press.</p>
</div>
<div id="ref-georgenorman87">
<p>Box, George E. P., and Norman R. Draper. 1987. <em>Empirical Model-Building and Response Surfaces</em>. New York: Wiley.</p>
</div>
<div id="ref-wilcox2012">
<p>Wilcox, Rand R. 2012. <em>Introduction to Robust Estimation and Hypothesis Testing</em>. 3rd;3; US: Academic Press.</p>
</div>
<div id="ref-Rawling98">
<p>Rawlings, John O., Sastry G. Pantula, and David A. Dickey. 1998. <em>Applied Regression Analysis: A Research Tool</em>. 2nd ed. New York: Springer.</p>
</div>
<div id="ref-R-sandwich">
<p>Lumley, Thomas, and Achim Zeileis. 2015. <em>Sandwich: Robust Covariance Matrix Estimators</em>. <a href="https://CRAN.R-project.org/package=sandwich" class="uri">https://CRAN.R-project.org/package=sandwich</a>.</p>
</div>
<div id="ref-obrien09">
<p>Obrien, Robert M. 2007. “A Caution Regarding Rules of Thumb for Variance Inflation Factors.” <em>Quality and Quantity</em> 41 (5).</p>
</div>
<div id="ref-gelman07">
<p>Gelman, Andrew, and Jennifer Hill. 2007. <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>. Cambridge;New York; Cambridge University Press.</p>
</div>
<div id="ref-R-visreg">
<p>Breheny, Patrick, and Woodrow Burchett. 2016. <em>Visreg: Visualization of Regression Models</em>. <a href="https://CRAN.R-project.org/package=visreg" class="uri">https://CRAN.R-project.org/package=visreg</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="17">
<li id="fn17"><p><a href="http://ww2.amstat.org/publications/jse/v9n3/stanton.html" class="uri">http://ww2.amstat.org/publications/jse/v9n3/stanton.html</a><a href="multiple-linear-regression-a-short-introduction.html#fnref17">↩</a></p></li>
<li id="fn18"><p>This might lead to a clue on popularity of controlled randomized trials.<a href="multiple-linear-regression-a-short-introduction.html#fnref18">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="correlation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="useful-r-codes.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/burakaydin/SARP-EN/tree/gh-pages/10_Regression.Rmd",
"text": "Edit"
},
"download": ["SARP-EN.pdf", "SARP-EN.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
